{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOVbE9rS3agc"
      },
      "source": [
        "# Задание 2.1 - Нейронные сети\n",
        "\n",
        "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
        "\n",
        "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
        "\n",
        "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nxqQATlr9pZL",
        "outputId": "9b825ac8-7e9a-467b-a4ca-ecd002035d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'dlcourse_ai' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sim0nsays/dlcourse_ai.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmK2OZRC9pxO"
      },
      "outputs": [],
      "source": [
        "!/content/dlcourse_ai/assignments/assignment1/download_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeZ7-iyJAin7"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/dlcourse_ai/assignments/assignment1/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26q7Bdnf9phf"
      },
      "outputs": [],
      "source": [
        "!cp /content/dlcourse_ai/assignments/assignment2/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpj6L2PZ3agf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8y_ly2Y3agh"
      },
      "outputs": [],
      "source": [
        "from dataset import load_svhn, random_split_train_val\n",
        "from optim import SGD, MomentumSGD\n",
        "from metrics import multiclass_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLAOLDIl3agh"
      },
      "source": [
        "# Загружаем данные\n",
        "\n",
        "И разделяем их на training и validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI6a7zdB3agi"
      },
      "outputs": [],
      "source": [
        "def prepare_for_neural_network(train_X, test_X):\n",
        "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
        "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
        "    \n",
        "    # Subtract mean\n",
        "    mean_image = np.mean(train_flat, axis = 0)\n",
        "    train_flat -= mean_image\n",
        "    test_flat -= mean_image\n",
        "    \n",
        "    return train_flat, test_flat\n",
        "    \n",
        "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
        "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
        "# Split train into train and val\n",
        "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alluOHc73agi"
      },
      "source": [
        "# Как всегда, начинаем с кирпичиков\n",
        "\n",
        "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
        "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
        "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
        "\n",
        "Начнем с ReLU, у которого параметров нет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yryxn76pCBZK"
      },
      "outputs": [],
      "source": [
        "class ReLULayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: Implement forward pass\n",
        "        # Hint: you'll need to save some information about X\n",
        "        # to use it later in the backward pass\n",
        "        self.X = X\n",
        "        return np.maximum(self.X, 0)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Arguments:\n",
        "        d_out, np array (batch_size, num_features) - gradient\n",
        "           of loss function with respect to output\n",
        "        Returns:\n",
        "        d_result: np array (batch_size, num_features) - gradient\n",
        "          with respect to input\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        # Your final implementation shouldn't have any loops\n",
        "        r = d_out.copy()\n",
        "        r[self.X <= 0] = 0\n",
        "        return r\n",
        "\n",
        "    def params(self):\n",
        "        # ReLU Doesn't have any parameters\n",
        "        return {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDnBddxQGl6Q"
      },
      "outputs": [],
      "source": [
        "def check_gradient(f, x, delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks the implementation of analytical gradient by comparing\n",
        "    it to numerical gradient using two-point formula\n",
        "    Arguments:\n",
        "      f: function that receives x and computes value and gradient\n",
        "      x: np array, initial point where gradient is checked\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Return: bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(x, np.ndarray)\n",
        "    assert x.dtype == float\n",
        "\n",
        "    orig_x = x.copy()\n",
        "    fx, analytic_grad = f(x)\n",
        "    assert np.all(np.isclose(orig_x, x, tol)), \"Functions shouldn't modify input variables\"\n",
        "\n",
        "    assert analytic_grad.shape == x.shape\n",
        "    analytic_grad = analytic_grad.copy()\n",
        "\n",
        "    # We will go through every dimension of x and compute numeric derivative for it\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "        analytic_grad_at_ix = analytic_grad[ix]\n",
        "\n",
        "        ix_delta = np.zeros_like(x)\n",
        "        ix_delta[ix] = delta\n",
        "        numeric_grad_at_ix = (f(x + ix_delta)[0] - f(x - ix_delta)[0]) / (2 * delta)\n",
        "\n",
        "        if not np.isclose(numeric_grad_at_ix, analytic_grad_at_ix, tol):\n",
        "            print(\"Gradients are different at %s. Analytic: %2.5f, Numeric: %2.5f\" % (\n",
        "            ix, analytic_grad_at_ix, numeric_grad_at_ix))\n",
        "            return False\n",
        "\n",
        "        it.iternext()\n",
        "\n",
        "    print(\"Gradient check passed!\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_layer_gradient(layer, x, delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for the input and output of a layer\n",
        "    Arguments:\n",
        "      layer: neural network layer, with forward and backward functions\n",
        "      x: starting point for layer input\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    output = layer.forward(x)\n",
        "    output_weight = np.random.randn(*output.shape)\n",
        "\n",
        "    def helper_func(x):\n",
        "        output = layer.forward(x)\n",
        "        loss = np.sum(output * output_weight)\n",
        "        d_out = np.ones_like(output) * output_weight\n",
        "        grad = layer.backward(d_out)\n",
        "        return loss, grad\n",
        "\n",
        "    return check_gradient(helper_func, x, delta, tol)\n",
        "\n",
        "\n",
        "def check_layer_param_gradient(layer, x,\n",
        "                               param_name,\n",
        "                               delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for the parameter of the layer\n",
        "    Arguments:\n",
        "      layer: neural network layer, with forward and backward functions\n",
        "      x: starting point for layer input\n",
        "      param_name: name of the parameter\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    param = layer.params()[param_name]\n",
        "    initial_w = param.value\n",
        "\n",
        "    output = layer.forward(x)\n",
        "    output_weight = np.random.randn(*output.shape)\n",
        "\n",
        "    def helper_func(w):\n",
        "        param.value = w\n",
        "        output = layer.forward(x)\n",
        "        loss = np.sum(output * output_weight)\n",
        "        d_out = np.ones_like(output) * output_weight\n",
        "        layer.backward(d_out)\n",
        "        grad = param.grad\n",
        "        return loss, grad\n",
        "\n",
        "    return check_gradient(helper_func, initial_w, delta, tol)\n",
        "\n",
        "\n",
        "def check_model_gradient(model, X, y,\n",
        "                         delta=1e-5, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Checks gradient correctness for all model parameters\n",
        "    Arguments:\n",
        "      model: neural network model with compute_loss_and_gradients\n",
        "      X: batch of input data\n",
        "      y: batch of labels\n",
        "      delta: step to compute numerical gradient\n",
        "      tol: tolerance for comparing numerical and analytical gradient\n",
        "    Returns:\n",
        "      bool indicating whether gradients match or not\n",
        "    \"\"\"\n",
        "    params = model.params()\n",
        "\n",
        "    for param_key in params:\n",
        "        print(\"Checking gradient for %s\" % param_key)\n",
        "        param = params[param_key]\n",
        "        initial_w = param.value\n",
        "\n",
        "        def helper_func(w):\n",
        "            param.value = w\n",
        "            loss = model.compute_loss_and_gradients(X, y)\n",
        "            grad = param.grad\n",
        "            return loss, grad\n",
        "\n",
        "        if not check_gradient(helper_func, initial_w, delta, tol):\n",
        "            return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WVdViCg3agj",
        "outputId": "8a3069b6-29e1-4603-bd80-cd1af70affc0",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement ReLULayer layer in layers.py\n",
        "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
        "\n",
        "X = np.array([[1,-2,3],\n",
        "              [-1, 2, 0.1]\n",
        "              ])\n",
        "\n",
        "assert check_layer_gradient(ReLULayer(), X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZqRXwTZ3agk"
      },
      "source": [
        "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
        "\n",
        "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
        "\n",
        "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdCVHx-xHMrJ"
      },
      "outputs": [],
      "source": [
        "def softmax(predictions):\n",
        "    \"\"\"\n",
        "    Computes probabilities from scores\n",
        "    Arguments:\n",
        "      predictions, np array, shape is either (N) or (batch_size, N) - classifier output\n",
        "    Returns:\n",
        "      probs, np array of the same shape as predictions - probability for every class, 0..1\n",
        "    \"\"\"\n",
        "\n",
        "    if predictions.ndim == 1:\n",
        "        normalized_predictions = predictions - np.max(predictions)\n",
        "        return np.exp(normalized_predictions) / np.sum(np.exp(normalized_predictions))\n",
        "    else:\n",
        "        normalized_predictions = predictions - np.max(predictions, axis=1)[:, np.newaxis]\n",
        "        return np.exp(normalized_predictions) / np.sum(np.exp(normalized_predictions), axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "def cross_entropy_loss(probabilities, target_index):\n",
        "    \"\"\"\n",
        "    Computes cross-entropy loss\n",
        "    Arguments:\n",
        "      probabilities, np array, shape is either (N) or (batch_size, N) - probabilities for every class\n",
        "      target_index: np array of int, shape is (1) or (batch_size) - index of the true class for given sample(s)\n",
        "    Returns:\n",
        "      loss: single value\n",
        "    \"\"\"\n",
        "    if probabilities.ndim == 1:\n",
        "        return -np.log(probabilities[target_index])\n",
        "    else:\n",
        "        flatten_target_index = target_index.flatten()\n",
        "        return -np.mean(np.log(probabilities[(\n",
        "            np.arange(flatten_target_index.shape[0]),\n",
        "            flatten_target_index\n",
        "        )]))\n",
        "\n",
        "\n",
        "def softmax_with_cross_entropy(predictions, target_index):\n",
        "    \"\"\"\n",
        "    Computes softmax and cross-entropy loss for model predictions,\n",
        "    including the gradient\n",
        "    Arguments:\n",
        "      predictions, np array, shape is either (N) or (batch_size, N) - classifier output\n",
        "      target_index: np array of int, shape is (1) or (batch_size) - index of the true class for given sample(s)\n",
        "    Returns:\n",
        "      loss, single value - cross-entropy loss\n",
        "      dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
        "    \"\"\"\n",
        "    probabilities = softmax(predictions)\n",
        "    loss = cross_entropy_loss(probabilities, target_index)\n",
        "\n",
        "    subtr = np.zeros_like(probabilities)\n",
        "\n",
        "    if probabilities.ndim == 1:\n",
        "        subtr[target_index] = 1\n",
        "        dprediction = probabilities - subtr\n",
        "    else:\n",
        "        batch_size = predictions.shape[0]\n",
        "        subtr[(\n",
        "            np.arange(target_index.shape[0]),\n",
        "            target_index.flatten()\n",
        "        )] = 1\n",
        "        dprediction = (probabilities - subtr) / batch_size\n",
        "\n",
        "    return loss, dprediction\n",
        "\n",
        "\n",
        "def l2_regularization(W, reg_strength):\n",
        "    \"\"\"\n",
        "    Computes L2 regularization loss on weights and its gradient\n",
        "    Arguments:\n",
        "      W, np array - weights\n",
        "      reg_strength - float value\n",
        "    Returns:\n",
        "      loss, single value - l2 regularization loss\n",
        "      gradient, np.array same shape as W - gradient of weight by l2 loss\n",
        "    \"\"\"\n",
        "\n",
        "    loss = reg_strength * (W ** 2).sum()\n",
        "    gradient = 2 * reg_strength * W\n",
        "\n",
        "    return loss, gradient\n",
        "\n",
        "\n",
        "def linear_softmax(X, W, target_index):\n",
        "    \"\"\"\n",
        "    Performs linear classification and returns loss and gradient over W\n",
        "    Arguments:\n",
        "      X, np array, shape (num_batch, num_features) - batch of images\n",
        "      W, np array, shape (num_features, classes) - weights\n",
        "      target_index, np array, shape (num_batch) - index of target classes\n",
        "    Returns:\n",
        "      loss, single value - cross-entropy loss\n",
        "      gradient, np.array same shape as W - gradient of weight by loss\n",
        "    \"\"\"\n",
        "    predictions = np.dot(X, W)\n",
        "\n",
        "    loss, dprediction = softmax_with_cross_entropy(predictions, target_index)\n",
        "    dW = np.dot(X.T, dprediction)\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "\n",
        "class Param:\n",
        "    \"\"\"\n",
        "    Trainable parameter of the model\n",
        "    Captures both parameter value and the gradient\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.grad = np.zeros_like(value)\n",
        "\n",
        "\n",
        "class ReLULayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: Implement forward pass\n",
        "        # Hint: you'll need to save some information about X\n",
        "        # to use it later in the backward pass\n",
        "        self.X = X\n",
        "        return np.maximum(self.X, 0)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Arguments:\n",
        "        d_out, np array (batch_size, num_features) - gradient\n",
        "           of loss function with respect to output\n",
        "        Returns:\n",
        "        d_result: np array (batch_size, num_features) - gradient\n",
        "          with respect to input\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        # Your final implementation shouldn't have any loops\n",
        "        r = d_out.copy()\n",
        "        r[self.X <= 0] = 0\n",
        "        return r\n",
        "\n",
        "    def params(self):\n",
        "        # ReLU Doesn't have any parameters\n",
        "        return {}\n",
        "\n",
        "\n",
        "class FullyConnectedLayer:\n",
        "    def __init__(self, n_input, n_output):\n",
        "        self.W = Param(0.001 * np.random.randn(n_input, n_output))\n",
        "        self.B = Param(0.001 * np.random.randn(1, n_output))\n",
        "        self.X = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        # TODO: Implement forward pass\n",
        "        # Your final implementation shouldn't have any loops\n",
        "        self.X = X\n",
        "        return self.X.dot(self.W.value) + self.B.value\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Computes gradient with respect to input and\n",
        "        accumulates gradients within self.W and self.B\n",
        "        Arguments:\n",
        "        d_out, np array (batch_size, n_output) - gradient\n",
        "           of loss function with respect to output\n",
        "        Returns:\n",
        "        d_result: np array (batch_size, n_input) - gradient\n",
        "          with respect to input\n",
        "        \"\"\"\n",
        "        # TODO: Implement backward pass\n",
        "        # Compute both gradient with respect to input\n",
        "        # and gradients with respect to W and B\n",
        "        # Add gradients of W and B to their `grad` attribute\n",
        "\n",
        "        # It should be pretty similar to linear classifier from\n",
        "        # the previous assignment\n",
        "\n",
        "        self.W.grad += self.X.T.dot(d_out)\n",
        "        self.B.grad += d_out.sum(axis=0, keepdims=0)\n",
        "\n",
        "        return d_out.dot(self.W.value.T)\n",
        "\n",
        "    def params(self):\n",
        "        return {'W': self.W, 'B': self.B}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNMspJbX3agl",
        "outputId": "57c48a05-a7b3-43a1-c79c-f6f4aabee20e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient check passed!\n",
            "Gradient check passed!\n",
            "Gradient check passed!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement FullyConnected layer forward and backward methods\n",
        "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
        "# TODO: Implement storing gradients for W and B\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
        "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uv_ZbeJ3agl"
      },
      "source": [
        "## Создаем нейронную сеть\n",
        "\n",
        "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
        "\n",
        "Не забудьте реализовать очистку градиентов в начале функции."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCnKRUjWHv-c"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet:\n",
        "    \"\"\" Neural network with two fully connected layers \"\"\"\n",
        "\n",
        "    def __init__(self, n_input, n_output, hidden_layer_size, reg):\n",
        "        \"\"\"\n",
        "        Initializes the neural network\n",
        "        Arguments:\n",
        "        n_input, int - dimension of the model input\n",
        "        n_output, int - number of classes to predict\n",
        "        hidden_layer_size, int - number of neurons in the hidden layer\n",
        "        reg, float - L2 regularization strength\n",
        "        \"\"\"\n",
        "        self.reg = reg\n",
        "        # TODO Create necessary layers\n",
        "        self.input_layer = FullyConnectedLayer(n_input, hidden_layer_size)\n",
        "        self.relu = ReLULayer()\n",
        "        self.output_layer = FullyConnectedLayer(hidden_layer_size, n_output)\n",
        "\n",
        "        self.W_in = self.input_layer.params()['W']\n",
        "        self.B_in = self.input_layer.params()['B']\n",
        "\n",
        "        self.W_out = self.output_layer.params()['W']\n",
        "        self.B_out = self.output_layer.params()['B']\n",
        "\n",
        "    def compute_loss_and_gradients(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes total loss and updates parameter gradients\n",
        "        on a batch of training examples\n",
        "        Arguments:\n",
        "        X, np array (batch_size, input_features) - input data\n",
        "        y, np array of int (batch_size) - classes\n",
        "        \"\"\"\n",
        "        # Before running forward and backward pass through the model,\n",
        "        # clear parameter gradients aggregated from the previous pass\n",
        "        # TODO Set parameter gradient to zeros\n",
        "        # Hint: using self.params() might be useful!\n",
        "        for p in self.params().values():\n",
        "            p.grad.fill(0)\n",
        "\n",
        "        # TODO Compute loss and fill param gradients\n",
        "        # by running forward and backward passes through the model\n",
        "\n",
        "        forward_input_layer = self.input_layer.forward(X)\n",
        "        forward_relu = self.relu.forward(forward_input_layer)\n",
        "        predictions = self.output_layer.forward(forward_relu)\n",
        "\n",
        "        loss, dprediction = softmax_with_cross_entropy(predictions, y)\n",
        "\n",
        "        backward_output_layer = self.output_layer.backward(dprediction)\n",
        "        backward_relu = self.relu.backward(backward_output_layer)\n",
        "        backward_input_layer = self.input_layer.backward(backward_relu)\n",
        "\n",
        "        # After that, implement l2 regularization on all params\n",
        "        # Hint: self.params() is useful again!\n",
        "\n",
        "        for p in self.params().values():\n",
        "            loss_l2, grad_l2 = l2_regularization(p.value, self.reg)\n",
        "            loss += loss_l2\n",
        "            p.grad += grad_l2\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Produces classifier predictions on the set\n",
        "        Arguments:\n",
        "          X, np array (test_samples, num_features)\n",
        "        Returns:\n",
        "          y_pred, np.array of int (test_samples)\n",
        "        \"\"\"\n",
        "        # TODO: Implement predict\n",
        "        # Hint: some of the code of the compute_loss_and_gradients\n",
        "        # can be reused\n",
        "\n",
        "        forward_input_layer = self.input_layer.forward(X)\n",
        "        forward_relu = self.relu.forward(forward_input_layer)\n",
        "        predictions = self.output_layer.forward(forward_relu)\n",
        "\n",
        "        return np.argmax(predictions, axis=-1)\n",
        "\n",
        "    def params(self):\n",
        "        # TODO Implement aggregating all of the params\n",
        "\n",
        "        return {\n",
        "            'W_in': self.W_in,\n",
        "            'B_in': self.B_in,\n",
        "            'W_out': self.W_out,\n",
        "            'B_out': self.B_out,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u39ahoVt3agm",
        "outputId": "940547a7-de6c-429d-beb6-7e967f196061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking gradient for W_in\n",
            "Gradient check passed!\n",
            "Checking gradient for B_in\n",
            "Gradient check passed!\n",
            "Checking gradient for W_out\n",
            "Gradient check passed!\n",
            "Checking gradient for B_out\n",
            "Gradient check passed!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: In model.py, implement compute_loss_and_gradients function\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
        "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
        "\n",
        "# TODO Now implement backward pass and aggregate all of the params\n",
        "check_model_gradient(model, train_X[:2], train_y[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n16DUvD73agm"
      },
      "source": [
        "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odB21f903agm",
        "outputId": "786600ae-06ab-49ec-c65a-7f30edf64dc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking gradient for W_in\n",
            "Gradient check passed!\n",
            "Checking gradient for B_in\n",
            "Gradient check passed!\n",
            "Checking gradient for W_out\n",
            "Gradient check passed!\n",
            "Checking gradient for B_out\n",
            "Gradient check passed!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO Now implement l2 regularization in the forward and backward pass\n",
        "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
        "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
        "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
        "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
        "\n",
        "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWuOT3EF3agn"
      },
      "source": [
        "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
        "\n",
        "Какое значение точности мы ожидаем увидеть до начала тренировки?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXEWGWsrNr0Q"
      },
      "outputs": [],
      "source": [
        "def multiclass_accuracy(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Computes metrics for multiclass classification\n",
        "    Arguments:\n",
        "    prediction, np array of int (num_samples) - model predictions\n",
        "    ground_truth, np array of int (num_samples) - true labels\n",
        "    Returns:\n",
        "    accuracy - ratio of accurate predictions to total samples\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement computing accuracy\n",
        "\n",
        "    return sum(np.equal(prediction, ground_truth)) / len(ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PhA-3Ro3agn",
        "outputId": "e24acb1e-2434-4667-ed97-ff24556f4649"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.13333333333333333"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Finally, implement predict function!\n",
        "\n",
        "# TODO: Implement predict function\n",
        "# What would be the value we expect?\n",
        "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg_wlO5p3agn"
      },
      "source": [
        "# Допишем код для процесса тренировки\n",
        "\n",
        "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-1LwUh0OSyY"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\"\n",
        "    Utility class to hold training and validation data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_X, train_y, val_X, val_y):\n",
        "        self.train_X = train_X\n",
        "        self.train_y = train_y\n",
        "        self.val_X = val_X\n",
        "        self.val_y = val_y\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Trainer of the neural network models\n",
        "    Perform mini-batch SGD with the specified data, model,\n",
        "    training parameters and optimization rule\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, dataset, optim,\n",
        "                 num_epochs=20,\n",
        "                 batch_size=20,\n",
        "                 learning_rate=1e-2,\n",
        "                 learning_rate_decay=1.0):\n",
        "        \"\"\"\n",
        "        Initializes the trainer\n",
        "        Arguments:\n",
        "        model - neural network model\n",
        "        dataset, instance of Dataset class - data to train on\n",
        "        optim - optimization method (see optim.py)\n",
        "        num_epochs, int - number of epochs to train\n",
        "        batch_size, int - batch size\n",
        "        learning_rate, float - initial learning rate\n",
        "        learning_rate_decal, float - ratio for decaying learning rate\n",
        "           every epoch\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.model = model\n",
        "        self.optim = optim\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate_decay = learning_rate_decay\n",
        "\n",
        "        self.optimizers = None\n",
        "\n",
        "    def setup_optimizers(self):\n",
        "        params = self.model.params()\n",
        "        self.optimizers = {}\n",
        "        for param_name, param in params.items():\n",
        "            self.optimizers[param_name] = deepcopy(self.optim)\n",
        "\n",
        "    def compute_accuracy(self, X, y):\n",
        "        \"\"\"\n",
        "        Computes accuracy on provided data using mini-batches\n",
        "        \"\"\"\n",
        "        indices = np.arange(X.shape[0])\n",
        "        sections = np.arange(self.batch_size, X.shape[0], self.batch_size)\n",
        "        batches_indices = np.array_split(indices, sections)\n",
        "\n",
        "        pred = np.zeros_like(y)\n",
        "\n",
        "        for batch_indices in batches_indices:\n",
        "            batch_X = X[batch_indices]\n",
        "            pred_batch = self.model.predict(batch_X)\n",
        "            pred[batch_indices] = pred_batch\n",
        "\n",
        "        return multiclass_accuracy(pred, y)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Trains a model\n",
        "        \"\"\"\n",
        "        if self.optimizers is None:\n",
        "            self.setup_optimizers()\n",
        "\n",
        "        num_train = self.dataset.train_X.shape[0]\n",
        "\n",
        "        loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            shuffled_indices = np.arange(num_train)\n",
        "            np.random.shuffle(shuffled_indices)\n",
        "            sections = np.arange(self.batch_size, num_train, self.batch_size)\n",
        "            batches_indices = np.array_split(shuffled_indices, sections)\n",
        "\n",
        "            batch_losses = []\n",
        "\n",
        "            for batch_indices in batches_indices:\n",
        "                # TODO Generate batches based on batch_indices and\n",
        "                # use model to generate loss and gradients for all\n",
        "                # the params\n",
        "\n",
        "                batch_X = self.dataset.train_X[batch_indices]\n",
        "                batch_y = self.dataset.train_y[batch_indices]\n",
        "\n",
        "                loss = self.model.compute_loss_and_gradients(batch_X, batch_y)\n",
        "\n",
        "                for param_name, param in self.model.params().items():\n",
        "                    optimizer = self.optimizers[param_name]\n",
        "                    param.value = optimizer.update(param.value, param.grad, self.learning_rate)\n",
        "\n",
        "                batch_losses.append(loss)\n",
        "\n",
        "            # TODO: Implement learning rate decay\n",
        "            self.learning_rate *= self.learning_rate_decay\n",
        "\n",
        "            ave_loss = np.mean(batch_losses)\n",
        "\n",
        "            train_accuracy = self.compute_accuracy(self.dataset.train_X, self.dataset.train_y)\n",
        "\n",
        "            val_accuracy = self.compute_accuracy(self.dataset.val_X, self.dataset.val_y)\n",
        "\n",
        "            print(f'Epoch: {epoch}, '\n",
        "                  f'loss: {\"{:.6f}\".format(batch_losses[-1])}, '\n",
        "                  f'train accuracy: {\"{:.6f}\".format(train_accuracy)}, '\n",
        "                  f'val accuracy: {\"{:.6f}\".format(val_accuracy)}'\n",
        "                  )\n",
        "\n",
        "            loss_history.append(ave_loss)\n",
        "            train_acc_history.append(train_accuracy)\n",
        "            val_acc_history.append(val_accuracy)\n",
        "\n",
        "        return loss_history, train_acc_history, val_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUKhlY6W3agn",
        "outputId": "28c06c24-350b-4f8f-932e-983036f62769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 2.302670, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 1, loss: 2.302285, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 2, loss: 2.302305, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 3, loss: 2.302563, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 4, loss: 2.302657, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 5, loss: 2.301966, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 6, loss: 2.302380, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 7, loss: 2.301363, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 8, loss: 2.301778, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 9, loss: 2.301804, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 10, loss: 2.302130, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 11, loss: 2.302631, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 12, loss: 2.302528, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 13, loss: 2.302254, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 14, loss: 2.301673, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 15, loss: 2.302299, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 16, loss: 2.302245, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 17, loss: 2.302372, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 18, loss: 2.302227, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 19, loss: 2.302673, train accuracy: 0.196667, val accuracy: 0.206000\n"
          ]
        }
      ],
      "source": [
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-3)\n",
        "\n",
        "# TODO Implement missing pieces in Trainer.fit function\n",
        "# You should expect loss to go down every epoch, even if it's slow\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "lz4L0ofJ3ago",
        "outputId": "b6c7da73-2237-4021-f89b-c7ae4d30e190"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f29337b9dd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(train_history)\n",
        "plt.plot(val_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9A8R9ta3ago"
      },
      "source": [
        "# Улучшаем процесс тренировки\n",
        "\n",
        "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFGJNFc3ago"
      },
      "source": [
        "## Уменьшение скорости обучения (learning rate decay)\n",
        "\n",
        "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
        "\n",
        "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
        "\n",
        "В нашем случае N будет равным 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "yp7TJ9lO3ago",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c5ea95-0cda-4377-b343-cc5161225b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 2.295309, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 1, loss: 2.254612, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 2, loss: 2.315275, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 3, loss: 2.274441, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 4, loss: 2.259460, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 5, loss: 2.287047, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 6, loss: 2.196892, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 7, loss: 2.267013, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 8, loss: 2.323265, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 9, loss: 2.328419, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 10, loss: 2.352009, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 11, loss: 2.092648, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 12, loss: 2.291146, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 13, loss: 2.250728, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 14, loss: 2.169802, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 15, loss: 2.222357, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 16, loss: 2.343212, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 17, loss: 2.317394, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 18, loss: 2.347696, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 19, loss: 2.322117, train accuracy: 0.196667, val accuracy: 0.206000\n"
          ]
        }
      ],
      "source": [
        "# TODO Implement learning rate decay inside Trainer.fit method\n",
        "# Decay should happen once per epoch\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
        "\n",
        "initial_learning_rate = trainer.learning_rate\n",
        "loss_history, train_history, val_history = trainer.fit()\n",
        "\n",
        "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
        "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppZGkhtE3agp"
      },
      "source": [
        "# Накопление импульса (Momentum SGD)\n",
        "\n",
        "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
        "\n",
        "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
        "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
        "\n",
        "```\n",
        "velocity = momentum * velocity - learning_rate * gradient \n",
        "w = w + velocity\n",
        "```\n",
        "\n",
        "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
        "\n",
        "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
        "http://cs231n.github.io/neural-networks-3/#sgd  \n",
        "https://distill.pub/2017/momentum/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MomentumSGD:\n",
        "    \"\"\"\n",
        "    Implements Momentum SGD update\n",
        "    \"\"\"\n",
        "    def __init__(self, momentum=0.9):\n",
        "        self.momentum = momentum\n",
        "        self.velocity = 0\n",
        "    \n",
        "    def update(self, w, d_w, learning_rate):\n",
        "        \"\"\"\n",
        "        Performs Momentum SGD update\n",
        "        Arguments:\n",
        "        w, np array - weights\n",
        "        d_w, np array, same shape as w - gradient\n",
        "        learning_rate, float - learning rate\n",
        "        Returns:\n",
        "        updated_weights, np array same shape as w\n",
        "        \"\"\"\n",
        "        # TODO Implement momentum update\n",
        "        # Hint: you'll need to introduce some variables to remember\n",
        "        # velocity from the previous updates\n",
        "\n",
        "        self.velocity = self.momentum * self.velocity - d_w * learning_rate\n",
        "\n",
        "        return w + self.velocity"
      ],
      "metadata": {
        "id": "9qSxd5veNOV6"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "gncaEWo43agp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08dcd879-bdce-488f-8af6-132f93493e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 2.322207, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 1, loss: 2.305562, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 2, loss: 2.312147, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 3, loss: 2.281875, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 4, loss: 2.287471, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 5, loss: 2.291925, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 6, loss: 2.295284, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 7, loss: 2.282446, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 8, loss: 2.330917, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 9, loss: 2.298601, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 10, loss: 2.282478, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 11, loss: 2.271572, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 12, loss: 2.284701, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 13, loss: 2.303934, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 14, loss: 2.245462, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 15, loss: 2.288840, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 16, loss: 2.284075, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 17, loss: 2.262612, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 18, loss: 2.310048, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 19, loss: 2.281977, train accuracy: 0.196667, val accuracy: 0.206000\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement MomentumSGD.update function in optim.py\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
        "\n",
        "# You should see even better results than before!\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofalk1bQ3agp"
      },
      "source": [
        "# Ну что, давайте уже тренировать сеть!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmfLX9603agp"
      },
      "source": [
        "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
        "\n",
        "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
        "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
        "\n",
        "Если этого не происходит, то где-то была допущена ошибка!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "CB-_nxDv3agq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b97ab1-b42a-477a-c051-c9c90204b925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 2.335821, train accuracy: 0.200000, val accuracy: 0.066667\n",
            "Epoch: 1, loss: 2.336464, train accuracy: 0.200000, val accuracy: 0.066667\n",
            "Epoch: 2, loss: 2.295228, train accuracy: 0.333333, val accuracy: 0.066667\n",
            "Epoch: 3, loss: 2.279056, train accuracy: 0.200000, val accuracy: 0.066667\n",
            "Epoch: 4, loss: 2.285170, train accuracy: 0.266667, val accuracy: 0.000000\n",
            "Epoch: 5, loss: 2.263880, train accuracy: 0.266667, val accuracy: 0.000000\n",
            "Epoch: 6, loss: 2.231426, train accuracy: 0.266667, val accuracy: 0.000000\n",
            "Epoch: 7, loss: 2.278779, train accuracy: 0.266667, val accuracy: 0.000000\n",
            "Epoch: 8, loss: 2.282354, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 9, loss: 2.246044, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 10, loss: 2.084230, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 11, loss: 2.021328, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 12, loss: 1.981028, train accuracy: 0.333333, val accuracy: 0.000000\n",
            "Epoch: 13, loss: 2.370104, train accuracy: 0.333333, val accuracy: 0.000000\n",
            "Epoch: 14, loss: 1.474613, train accuracy: 0.333333, val accuracy: 0.000000\n",
            "Epoch: 15, loss: 2.109365, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 16, loss: 1.828111, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 17, loss: 2.464465, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 18, loss: 1.520504, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 19, loss: 1.592158, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 20, loss: 1.827717, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 21, loss: 2.334058, train accuracy: 0.466667, val accuracy: 0.066667\n",
            "Epoch: 22, loss: 2.353732, train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Epoch: 23, loss: 1.709371, train accuracy: 0.400000, val accuracy: 0.066667\n",
            "Epoch: 24, loss: 1.571413, train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Epoch: 25, loss: 2.079682, train accuracy: 0.466667, val accuracy: 0.066667\n",
            "Epoch: 26, loss: 2.142111, train accuracy: 0.533333, val accuracy: 0.066667\n",
            "Epoch: 27, loss: 1.835104, train accuracy: 0.466667, val accuracy: 0.066667\n",
            "Epoch: 28, loss: 2.006710, train accuracy: 0.466667, val accuracy: 0.066667\n",
            "Epoch: 29, loss: 1.977381, train accuracy: 0.466667, val accuracy: 0.066667\n",
            "Epoch: 30, loss: 2.191123, train accuracy: 0.466667, val accuracy: 0.000000\n",
            "Epoch: 31, loss: 1.402382, train accuracy: 0.466667, val accuracy: 0.066667\n",
            "Epoch: 32, loss: 1.780685, train accuracy: 0.533333, val accuracy: 0.066667\n",
            "Epoch: 33, loss: 1.396967, train accuracy: 0.533333, val accuracy: 0.066667\n",
            "Epoch: 34, loss: 1.673215, train accuracy: 0.600000, val accuracy: 0.066667\n",
            "Epoch: 35, loss: 2.183524, train accuracy: 0.533333, val accuracy: 0.066667\n",
            "Epoch: 36, loss: 1.459016, train accuracy: 0.533333, val accuracy: 0.000000\n",
            "Epoch: 37, loss: 2.350079, train accuracy: 0.600000, val accuracy: 0.066667\n",
            "Epoch: 38, loss: 1.125837, train accuracy: 0.600000, val accuracy: 0.066667\n",
            "Epoch: 39, loss: 1.533345, train accuracy: 0.600000, val accuracy: 0.000000\n",
            "Epoch: 40, loss: 1.603820, train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Epoch: 41, loss: 1.260938, train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Epoch: 42, loss: 1.398117, train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Epoch: 43, loss: 1.793744, train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Epoch: 44, loss: 1.338260, train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Epoch: 45, loss: 2.161183, train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Epoch: 46, loss: 1.292675, train accuracy: 0.666667, val accuracy: 0.000000\n",
            "Epoch: 47, loss: 1.508914, train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Epoch: 48, loss: 1.637823, train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Epoch: 49, loss: 1.350498, train accuracy: 0.600000, val accuracy: 0.066667\n",
            "Epoch: 50, loss: 0.953936, train accuracy: 0.666667, val accuracy: 0.066667\n",
            "Epoch: 51, loss: 1.491992, train accuracy: 0.666667, val accuracy: 0.133333\n",
            "Epoch: 52, loss: 1.661981, train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Epoch: 53, loss: 1.736327, train accuracy: 0.733333, val accuracy: 0.133333\n",
            "Epoch: 54, loss: 1.521500, train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Epoch: 55, loss: 1.699028, train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Epoch: 56, loss: 2.069429, train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Epoch: 57, loss: 1.240860, train accuracy: 0.733333, val accuracy: 0.066667\n",
            "Epoch: 58, loss: 2.003166, train accuracy: 0.733333, val accuracy: 0.133333\n",
            "Epoch: 59, loss: 1.536247, train accuracy: 0.733333, val accuracy: 0.133333\n",
            "Epoch: 60, loss: 1.226104, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 61, loss: 1.324674, train accuracy: 0.800000, val accuracy: 0.133333\n",
            "Epoch: 62, loss: 1.318346, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 63, loss: 1.526520, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 64, loss: 1.689462, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 65, loss: 2.236853, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 66, loss: 1.479916, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 67, loss: 1.683291, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 68, loss: 1.401988, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 69, loss: 1.613544, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 70, loss: 1.325859, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 71, loss: 1.330843, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 72, loss: 1.236715, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 73, loss: 1.203392, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 74, loss: 1.671326, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 75, loss: 1.594131, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 76, loss: 1.344221, train accuracy: 0.800000, val accuracy: 0.066667\n",
            "Epoch: 77, loss: 1.479204, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 78, loss: 1.085193, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 79, loss: 0.947806, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 80, loss: 1.451013, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 81, loss: 1.331704, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 82, loss: 1.379418, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 83, loss: 2.059189, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 84, loss: 1.344211, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 85, loss: 1.665988, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 86, loss: 1.418788, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 87, loss: 1.668223, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 88, loss: 1.889122, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 89, loss: 1.279486, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 90, loss: 1.170461, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 91, loss: 1.420741, train accuracy: 0.866667, val accuracy: 0.000000\n",
            "Epoch: 92, loss: 1.192341, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 93, loss: 1.511670, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 94, loss: 1.033806, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 95, loss: 1.279162, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 96, loss: 1.402222, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 97, loss: 0.948651, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 98, loss: 1.384973, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 99, loss: 1.418985, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 100, loss: 1.635383, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 101, loss: 1.534292, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 102, loss: 1.045888, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 103, loss: 1.526229, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 104, loss: 1.769138, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 105, loss: 1.066875, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 106, loss: 1.089929, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 107, loss: 1.483974, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 108, loss: 1.243191, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 109, loss: 1.281256, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 110, loss: 1.061273, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 111, loss: 1.281814, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 112, loss: 1.441965, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 113, loss: 1.110291, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 114, loss: 1.243326, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 115, loss: 1.399829, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 116, loss: 1.086163, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 117, loss: 1.313868, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 118, loss: 1.234330, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 119, loss: 1.233903, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 120, loss: 1.269580, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 121, loss: 1.380817, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 122, loss: 1.388071, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 123, loss: 1.028628, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 124, loss: 1.194766, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 125, loss: 1.304665, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 126, loss: 1.282250, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 127, loss: 1.417781, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 128, loss: 1.191691, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 129, loss: 1.399456, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 130, loss: 1.102942, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 131, loss: 1.359230, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 132, loss: 1.390727, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 133, loss: 1.415042, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 134, loss: 1.168363, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 135, loss: 1.110167, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 136, loss: 1.261038, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 137, loss: 1.414528, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 138, loss: 1.441156, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 139, loss: 1.098176, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 140, loss: 1.257759, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 141, loss: 1.379033, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 142, loss: 1.657464, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 143, loss: 1.338941, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 144, loss: 1.517630, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 145, loss: 1.103833, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 146, loss: 1.241927, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 147, loss: 1.058143, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 148, loss: 1.556551, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 149, loss: 1.274176, train accuracy: 1.000000, val accuracy: 0.000000\n"
          ]
        }
      ],
      "source": [
        "data_size = 15\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
        "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
        "\n",
        "# You should expect this to reach 1.0 training accuracy \n",
        "loss_history, train_history, val_history = trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QisVPthC3agq"
      },
      "source": [
        "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
        "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
        "Найдите их!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "aj9Y9WsN3agq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7205637e-1087-4a9f-eecc-c7d3968ba1e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 2.320141, train accuracy: 0.266667, val accuracy: 0.066667\n",
            "Epoch: 1, loss: 2.216322, train accuracy: 0.200000, val accuracy: 0.066667\n",
            "Epoch: 2, loss: 2.445361, train accuracy: 0.333333, val accuracy: 0.066667\n",
            "Epoch: 3, loss: 2.343844, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 4, loss: 2.197360, train accuracy: 0.333333, val accuracy: 0.133333\n",
            "Epoch: 5, loss: 2.140217, train accuracy: 0.400000, val accuracy: 0.000000\n",
            "Epoch: 6, loss: 1.558163, train accuracy: 0.533333, val accuracy: 0.133333\n",
            "Epoch: 7, loss: 2.377700, train accuracy: 0.600000, val accuracy: 0.000000\n",
            "Epoch: 8, loss: 1.798986, train accuracy: 0.600000, val accuracy: 0.133333\n",
            "Epoch: 9, loss: 0.139595, train accuracy: 0.533333, val accuracy: 0.066667\n",
            "Epoch: 10, loss: 0.093301, train accuracy: 0.733333, val accuracy: 0.000000\n",
            "Epoch: 11, loss: 0.083112, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 12, loss: 0.031759, train accuracy: 0.800000, val accuracy: 0.133333\n",
            "Epoch: 13, loss: 0.018033, train accuracy: 0.800000, val accuracy: 0.133333\n",
            "Epoch: 14, loss: 1.200484, train accuracy: 0.800000, val accuracy: 0.000000\n",
            "Epoch: 15, loss: 0.222009, train accuracy: 0.933333, val accuracy: 0.066667\n",
            "Epoch: 16, loss: 0.859929, train accuracy: 0.933333, val accuracy: 0.000000\n",
            "Epoch: 17, loss: 0.573076, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 18, loss: 0.050690, train accuracy: 1.000000, val accuracy: 0.000000\n",
            "Epoch: 19, loss: 0.336544, train accuracy: 1.000000, val accuracy: 0.000000\n"
          ]
        }
      ],
      "source": [
        "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
        "\n",
        "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 400, reg = 1e-3)\n",
        "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
        "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
        "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=2)\n",
        "\n",
        "loss_history, train_history, val_history = trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP9k-alE3agq"
      },
      "source": [
        "# Итак, основное мероприятие!\n",
        "\n",
        "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
        "\n",
        "Добейтесь точности лучше **60%** на validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "P83eRTag3agr",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04093d09-7ede-4520-a24f-8afb9b055914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 2.226307, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 1, loss: 2.031843, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 2, loss: 1.907048, train accuracy: 0.313889, val accuracy: 0.322000\n",
            "Epoch: 3, loss: 1.802091, train accuracy: 0.432667, val accuracy: 0.438000\n",
            "Epoch: 4, loss: 1.551115, train accuracy: 0.535111, val accuracy: 0.533000\n",
            "Epoch: 5, loss: 1.690280, train accuracy: 0.606889, val accuracy: 0.594000\n",
            "Epoch: 6, loss: 1.192438, train accuracy: 0.655111, val accuracy: 0.643000\n",
            "Epoch: 7, loss: 1.082381, train accuracy: 0.661333, val accuracy: 0.631000\n",
            "Epoch: 8, loss: 1.217155, train accuracy: 0.700556, val accuracy: 0.671000\n",
            "Epoch: 9, loss: 1.060865, train accuracy: 0.704000, val accuracy: 0.664000\n",
            "Epoch: 10, loss: 0.890894, train accuracy: 0.716667, val accuracy: 0.681000\n",
            "Epoch: 11, loss: 1.202148, train accuracy: 0.727778, val accuracy: 0.691000\n",
            "Epoch: 12, loss: 1.151817, train accuracy: 0.746444, val accuracy: 0.712000\n",
            "Epoch: 13, loss: 0.979458, train accuracy: 0.743889, val accuracy: 0.710000\n",
            "Epoch: 14, loss: 0.842482, train accuracy: 0.751667, val accuracy: 0.712000\n",
            "Epoch: 15, loss: 1.277744, train accuracy: 0.758111, val accuracy: 0.712000\n",
            "Epoch: 16, loss: 1.107261, train accuracy: 0.769111, val accuracy: 0.720000\n",
            "Epoch: 17, loss: 1.059776, train accuracy: 0.772444, val accuracy: 0.723000\n",
            "Epoch: 18, loss: 0.849432, train accuracy: 0.778444, val accuracy: 0.728000\n",
            "Epoch: 19, loss: 1.164141, train accuracy: 0.779333, val accuracy: 0.733000\n",
            "Best learning suite: best learning rate: 0.1 best regularization strength: 0.001 best learning rate decay: 0.9 best hidden layer size: 256 best batch size: 256 \n",
            "Best validation accuracy: 0.733\n",
            "Epoch: 0, loss: 2.285164, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 1, loss: 2.053660, train accuracy: 0.228556, val accuracy: 0.239000\n",
            "Epoch: 2, loss: 1.830476, train accuracy: 0.367667, val accuracy: 0.359000\n",
            "Epoch: 3, loss: 1.490232, train accuracy: 0.441556, val accuracy: 0.472000\n",
            "Epoch: 4, loss: 1.534042, train accuracy: 0.564111, val accuracy: 0.553000\n",
            "Epoch: 5, loss: 1.444715, train accuracy: 0.620556, val accuracy: 0.594000\n",
            "Epoch: 6, loss: 1.107654, train accuracy: 0.643889, val accuracy: 0.628000\n",
            "Epoch: 7, loss: 0.830781, train accuracy: 0.681889, val accuracy: 0.662000\n",
            "Epoch: 8, loss: 1.243948, train accuracy: 0.711000, val accuracy: 0.683000\n",
            "Epoch: 9, loss: 1.196135, train accuracy: 0.705333, val accuracy: 0.674000\n",
            "Epoch: 10, loss: 0.911776, train accuracy: 0.744556, val accuracy: 0.712000\n",
            "Epoch: 11, loss: 1.125312, train accuracy: 0.749889, val accuracy: 0.706000\n",
            "Epoch: 12, loss: 1.064934, train accuracy: 0.747778, val accuracy: 0.706000\n",
            "Epoch: 13, loss: 1.246529, train accuracy: 0.762222, val accuracy: 0.722000\n",
            "Epoch: 14, loss: 0.898303, train accuracy: 0.759556, val accuracy: 0.712000\n",
            "Epoch: 15, loss: 0.861584, train accuracy: 0.774556, val accuracy: 0.727000\n",
            "Epoch: 16, loss: 1.179878, train accuracy: 0.784556, val accuracy: 0.735000\n",
            "Epoch: 17, loss: 0.929685, train accuracy: 0.789222, val accuracy: 0.732000\n",
            "Epoch: 18, loss: 1.041469, train accuracy: 0.793444, val accuracy: 0.736000\n",
            "Epoch: 19, loss: 0.803740, train accuracy: 0.794000, val accuracy: 0.737000\n",
            "Best learning suite: best learning rate: 0.1 best regularization strength: 0.001 best learning rate decay: 0.9 best hidden layer size: 512 best batch size: 256 \n",
            "Best validation accuracy: 0.737\n",
            "Epoch: 0, loss: 2.278517, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 1, loss: 2.229256, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 2, loss: 2.235528, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 3, loss: 2.233289, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 4, loss: 2.212546, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 5, loss: 2.213124, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 6, loss: 2.295004, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 7, loss: 2.237416, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 8, loss: 2.106983, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 9, loss: 2.252711, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 10, loss: 2.281518, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 11, loss: 2.238608, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 12, loss: 2.176226, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 13, loss: 2.228207, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 14, loss: 2.211953, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 15, loss: 2.225559, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 16, loss: 2.206610, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 17, loss: 2.182373, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 18, loss: 2.371934, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 19, loss: 2.172502, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Best learning suite: best learning rate: 0.1 best regularization strength: 0.001 best learning rate decay: 0.9 best hidden layer size: 512 best batch size: 256 \n",
            "Best validation accuracy: 0.737\n",
            "Epoch: 0, loss: 2.259427, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 1, loss: 2.220371, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 2, loss: 2.175138, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 3, loss: 2.238218, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 4, loss: 2.245512, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 5, loss: 2.129791, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 6, loss: 2.164487, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 7, loss: 2.270990, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 8, loss: 2.219012, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 9, loss: 2.229735, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 10, loss: 2.198447, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 11, loss: 2.186217, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 12, loss: 2.135658, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 13, loss: 2.164828, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 14, loss: 2.133761, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 15, loss: 2.226682, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 16, loss: 2.254407, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 17, loss: 2.300441, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 18, loss: 2.170461, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Epoch: 19, loss: 2.303354, train accuracy: 0.196667, val accuracy: 0.206000\n",
            "Best learning suite: best learning rate: 0.1 best regularization strength: 0.001 best learning rate decay: 0.9 best hidden layer size: 512 best batch size: 256 \n",
            "Best validation accuracy: 0.737\n",
            "Best validation accuracy achieved: 0.737\n"
          ]
        }
      ],
      "source": [
        "# Let's train the best one-hidden-layer network we can\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "learning_rates = [1e-1, 1e-2]\n",
        "reg_strength = [1e-3]\n",
        "learning_rate_decay = [0.9]\n",
        "hidden_layer_size = [256, 512]\n",
        "num_epochs = 20\n",
        "batch_size = [256]\n",
        "\n",
        "best_classifier = None\n",
        "best_val_accuracy = 0\n",
        "\n",
        "loss_history = []\n",
        "train_history = []\n",
        "val_history = []\n",
        "\n",
        "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
        "\n",
        "# TODO find the best hyperparameters to train the network\n",
        "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
        "# You should expect to get to at least 40% of valudation accuracy\n",
        "# Save loss/train/history of the best classifier to the variables above\n",
        "\n",
        "learning_suites = [\n",
        "    learning_rates,\n",
        "    reg_strength,\n",
        "    learning_rate_decay,\n",
        "    hidden_layer_size,\n",
        "    batch_size\n",
        "]\n",
        "\n",
        "best_learning_suite = None\n",
        "\n",
        "for lr, rs, lrd, hls, bs in product(*learning_suites):\n",
        "    model = TwoLayerNet(n_input=train_X.shape[1],\n",
        "                        n_output=10,\n",
        "                        hidden_layer_size=hls,\n",
        "                        reg=rs\n",
        "                        )\n",
        "    trainer = Trainer(model, dataset, MomentumSGD(),\n",
        "                      learning_rate=lr,\n",
        "                      learning_rate_decay=lrd,\n",
        "                      num_epochs=num_epochs,\n",
        "                      batch_size=bs\n",
        "                      )\n",
        "    ls_loss_history, ls_train_history, ls_val_history = trainer.fit()\n",
        "\n",
        "    if ls_val_history[-1] > best_val_accuracy:\n",
        "        best_classifier = model\n",
        "        best_val_accuracy = ls_val_history[-1]\n",
        "        best_learning_suite = [(lr, rs, lrd, hls, bs)]\n",
        "        loss_history = ls_loss_history.copy()\n",
        "        train_history = ls_train_history.copy()\n",
        "        val_history = ls_val_history.copy()\n",
        "\n",
        "    for best_lr, best_rs, best_lrd, best_hls, best_bs in best_learning_suite:\n",
        "        print(f'Best learning suite: '\n",
        "              f'best learning rate: {best_lr} '\n",
        "              f'best regularization strength: {best_rs} '\n",
        "              f'best learning rate decay: {best_lrd} '\n",
        "              f'best hidden layer size: {best_hls} '\n",
        "              f'best batch size: {best_bs} '\n",
        "              )\n",
        "    print(f'Best validation accuracy: {best_val_accuracy}')\n",
        "\n",
        "print(f'Best validation accuracy achieved: {best_val_accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "T8z9bZj13agr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "5c55c09b-5107-4c75-8196-9007fdf93dca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f29336e7790>]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "plt.subplot(211)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(loss_history)\n",
        "plt.subplot(212)\n",
        "plt.title(\"Train/validation accuracy\")\n",
        "plt.plot(train_history)\n",
        "plt.plot(val_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0wu4WYp3agr"
      },
      "source": [
        "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "MEhBd7el3agr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3efe074-f7cb-4404-c7a9-f69c70e27678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural net test set accuracy: 0.707000\n"
          ]
        }
      ],
      "source": [
        "test_pred = best_classifier.predict(test_X)\n",
        "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
        "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Копия блокнота \"Neural Network.ipynb\"",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}